---
title: "batches_correlation_analysis"
author: "Johanna Mehler"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Packages

```{r}
# libraries
library(tidyverse)
library(corrplot)
library(corrr)
library(GGally)
```
## Read in Data

```{r}
# read in batches
batches <- read_csv("/Users/Jo/OneDrive/1_Hertie Studies/Thesis/Hertie-Thesis-Mehler/data/batches.csv")
```

# Pre-Processing

Create numeric Values (TRUE = 1, FALSE = 0) for all columns and remove the country column for now. Create dataframes for each category content, target etc.

```{r}
# Convert boolean (logical) columns to numeric
data_numeric <- batches %>% 
  select(-ends_with("open"), -Answer.flag_comments, -Input.country) %>% 
  mutate(across(.cols = everything(), .fns = ~ as.numeric(. == TRUE)))

# take out answers that are flagged for revision
data_numeric_filtered <- data_numeric %>% filter(Answer.flag.flag == FALSE)

# create subsets for each category
data_content <- data_numeric %>% select(starts_with("Answer.content")) # 15
data_other_features <- data_numeric %>% select(starts_with("Answer.other")) # 7
data_scope <- data_numeric %>% select(starts_with("Answer.scope")) # 2
data_sender <- data_numeric %>% select(starts_with("Answer.sender")) # 5
data_target <- data_numeric %>% select(starts_with("Answer.target")) # 18
```

Check for columns with no variability (problem for calculating correlation)

```{r}
# Function to check and report columns with only zeros
report_columns_with_only_zeros <- function(data) {
  # Identify columns with only zeros
  zero_var_cols <- apply(data, 2, function(column) all(column == 0))
  
  # Names of columns with only zeros
  columns_with_zeros <- names(data)[zero_var_cols]
  
  if(length(columns_with_zeros) > 0) {
    cat("Columns with only zeros:", paste(columns_with_zeros, collapse = ", "), "\n")
  } else {
    cat("No columns with only zeros found.\n")
  }
}

# Using the function with the data_numeric dataset
report_columns_with_only_zeros(data_numeric)
```

# Frequences

## Overall Patterns

```{r}
# Overall patterns: Count the frequency of TRUE values for each Answer.* column
overall_patterns <- colSums(data_numeric_filtered, na.rm = TRUE) # Assuming the first column is `Input.country`
overall_patterns_sorted <- sort(overall_patterns, decreasing = TRUE)

# Display the most common properties/content
head(overall_patterns_sorted, 10)
```
These results indicate that the most common types of content in responses involve attacks, incitement, and hateful remarks, often targeting race or religion and focusing on either individuals or groups. The share of nonsense answers seems big and should be further investigated, especially in terms of a potential underlying political attitude.


## Country-Specific Patterns

```{r}
# add country column to the dataframe again - but needs to have same amount of rows...
data_numeric_country <- data_numeric %>% mutate(Input.country = batches$Input.country)

country_specific_patterns <- data_numeric_country %>%
  group_by(Input.country) %>%
  summarise(across(starts_with("Answer."), sum, na.rm = TRUE))

# Preview the results for a few countries
head(country_specific_patterns)
```
Numbers BEFORE filtering, need to be adjusted

* Great Britain (gbr): High frequency of attack content (116), incitement content (66), and a notable focus on hateful content (48).
* Germany (ger): Similar to Great Britain in terms of attack content (111) but with a higher frequency of dehumanization content (49) and incitement content (72).
* India (English responses, indeng): Shows a lower frequency of attack content (59) compared to GB and Germany, with a significant focus on religion as a target (55).
* Nigeria (nig): Notable for high dehumanization content (33) and a focus on race (51) and religion (45) as targets.
* Philippines (English responses, phleng): Similar to Nigeria in terms of dehumanization content (32) and a focus on hateful content (34).

## Identifying Common Combinations

```{r}
# Add a unique row identifier to the dataset in order to group in long format later
data_numeric_filtered_ID <- data_numeric_filtered %>%
  mutate(id = row_number())

# Convert data frame to long format
data_long <- pivot_longer(data_numeric_filtered_ID, cols = starts_with("Answer."), names_to = "variable", values_to = "value")

# Filter for TRUE values only
data_true <- filter(data_long, value == TRUE)

# Adjusted code to skip groups with fewer than 2 items
data_pairs <- data_true %>%
  group_by(id) %>%
  filter(n() >= 2) %>% # Ensure group has at least 2 items
  summarise(pairs = list(combn(variable, 2, simplify = FALSE))) %>%
  unnest(pairs) %>%
  ungroup() %>%
  mutate(pairs = map_chr(pairs, ~paste(sort(.x), collapse = " & "))) %>%
  count(pairs, sort = TRUE)

# Display the most common pairs
head(data_pairs)
```
INTERPRETATION

* people and person are mentioned a lot together as the scope of hate speech targets
* attacking a person or people is maybe the best "common ground"
* race and religion are mostly mentioned together as two potential target characteristics

```{r}
# Adjusted code to create triples
data_triples <- data_true %>%
  group_by(id) %>%
  filter(n() >= 3) %>% # Ensure group has at least 2 items
  summarise(triples = list(combn(variable, 3, simplify = FALSE))) %>%
  unnest(triples) %>%
  ungroup() %>%
  mutate(triples = map_chr(triples, ~paste(sort(.x), collapse = " & "))) %>%
  count(triples, sort = TRUE)

# Display the most common pairs
head(data_triples)
```
INTERPRETATION

* attack on people or person -> same as in pairs
* people as scope, race and religion (and sex / sexual orientation) as targets -> this is basically the most common definition, also in the literature

# Correlation-Matrices

```{r}
# too big and messy
# data_numeric %>% cor() %>% corrplot(method = "circle")

# check if all variables have some variability and are not completely zero

data_list <- list(
  content = data_content,
  other_features = data_other_features,
  scope = data_scope,
  sender = data_sender,
  target = data_target
)

subset_names <- names(data_list)
combinations <- combn(subset_names, 2)

compute_and_plot_cor <- function(subset1, subset2) {
  combined_data <- bind_cols(data_list[[subset1]], data_list[[subset2]])
  
  # Ensure data is numeric and remove columns with zero variance
  combined_data <- combined_data %>% 
    mutate(across(everything(), as.numeric)) %>%
    select_if(~ var(.) != 0)
  
  # Check if there are enough columns with variance for correlation analysis
  if(ncol(combined_data) > 1) {
    cor_matrix <- cor(combined_data, use = "complete.obs")
    corrplot(cor_matrix, method = "circle", title = paste("Correlation between", subset1, "and", subset2))
  } else {
    cat("Not enough variability between", subset1, "and", subset2, "for correlation analysis.\n")
  }
}

apply(combinations, 2, function(pair) compute_and_plot_cor(pair[1], pair[2]))

```


# Other approach for Correlation-Matrix 

```{r}
cor_matrix <- data_numeric %>% correlate()
cor_matrix
```

# Plotting the Correlation Matrix

```{r}
cor_plot <- data_numeric %>% GGally::ggpairs()
cor_plot
```



